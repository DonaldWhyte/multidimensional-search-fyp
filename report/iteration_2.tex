\section{Iteration \#2 -- Pyramid Tree and Other Hash-Based Approaches}

This iteration explores other hashing functions to find a hash-based index structure which performs well on the two scientific datasets.

\subsection{Pyramid Tree}

The Pyramid Tree implementation is identical to the Pseudo-Pyramid Tree implementation, except points are now hashed to their pyramid value as described in Section \ref{sec:pyramid-tree-detail}.

\subsection{B${}^+$-Tree as Underlying Structure}

Berchtold et al. originally used a B${}^{+}$-tree when developing the Pyramid Tree \cite{pyramid-tree}. It was decided that using the same underlying search structure would allow for a more fair comparison of the Pyramid tree data structure. Two B${}^{+}$-tree implementations, bpt\cite{bpt} and cpp-btree\cite{cpp-btree}, were used as the underlying structure for the Pyramid Tree. The first is a C++ implementation while the second is a pure C implementation.

Table \ref{tab:hashtable-bplus-time-comparison} provides total execution times of each operation, measured using Insert-Query-Delete operation list with 500,000 points from the 16D uniform synthetic dataset. There is a substantial difference between the speed of the hash table Pyramid tree and the B${}^{+}$-tree implementations. After profiling, it was discovered the main cause of the decrease in speed was simply the additional overhead incurred by splitting and merging nodes in the B${}^{+}$-tree. This matches the theoretical performance analyses of the two structures, where it is shown that hash tables and B${}^{+}$-trees have amortised $O(1)$ and $O(\log n)$ operations respectively (the former being dependant on the hashing function used).

Based on these results, it has been decided to continue using the hash table, not the B${}^{+}$-tree, as the underlying search structure.

\begin{table}
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		\textbf{Operation} & \texttt{boost::unordered\_map} & bpt & cpp-btree  \\
		\hline
		\textbf{Delete} & 0.211423 & 0.450339 & 0.975521 \\
		\textbf{Insert} & 0.379671 & 0.680246 & 1.278914 \\
		\textbf{Point Query} & 0.177153 & 0.259851 & 0.548411 \\
		\hline
	\end{tabular}
	\caption{Total Execution Time (in Seconds) of Pseudo-Pyramid Tree With Different Underlying 1D Index Structures (16D Randomly Uniform Dataset, 500,000 Points)}
	\label{tab:hashtable-bplus-time-comparison}
\end{table}

\subsection{Bucket Hash Table}

% http://stackoverflow.com/questions/7403210/hashing-floating-point-values
% https://svn.boost.org/trac/boost/ticket/4038
% http://programmers.stackexchange.com/questions/63595/0x9e3779b9-golden-number
% http://stackoverflow.com/questions/4948780/magic-number-in-boosthash-combine
% http://burtleburtle.net/bob/hash/doobs.html

If larger bucket sizes mean more point comparisons are performed on average, then it is not unreasonable to expect an average bucket size of 1 to provide very good performance. If bucket size is one and the hashing function is an $O(d)$ operation, then each operation will only take $O(d)$ time, regardless of dataset size.

The Bucket Hash Table is a structure which uses the same underlying implemention as a Bucket Pseudo-Pyramid Tree. However, instead of trying to exploit the spatial properties of points directly to provide good bucket size, a different, more general-purpose hashing function provided in the Boost library is used. Like the other hash-based structures discussed in this section, it is possible for two points to be hashed to the same value. However, from empirical performance tests, it has been shown that the average bucket size is almost always near one (bucket size measurements are provided in the next section), meaning most operations are performed in $O(d)$ time.

A high-level algorithm describing the hashing function is given in Algorithm \ref{alg:point-hashing} in Appendix \ref{chap:supp-material}. This function hashes each individual coordinate (floating point value) and combines them using exclusive-or ($\oplus$) and bitshifting operations. A magic number representing the reciprocal of the golden ratio, $\phi = \frac{1 + \sqrt{5}}{2}$, is used when combining the hash values of individual coordinates. The choice to use the golden ratio was inspired by Jenkins' hash function\cite{hash-combine}, where it is used to ensure consecutive floating point values will be mapped to integers with large distances between them. This increases the likelihood of having points distributed more uniformly across buckets when points are clustered within a small numerical range (like they are in the astrophysics dataset).

One major issue with this approach is the potential for floating-point inaccuracy to give incorrect results. On the controlled performance tests executed in this project, a point is queried using the exact same floating point values for the coordinates as when it was inserted. This means the two points have the same identical bit patterns. 

Suppose the point to query was the output of a more complex computation, where rounding errors may come into play. Even if an equal point conceptually is being stored in the structure, rounding errors may cause the two points to have different bit patterns, potentially resulting in different hashed values. The output point, while being stored in the structure, will appear as if it is not. This is a common issue when using floating point values in hashing functions.

Therefore, the Bucket Hash Table may be unreliable for certain applications, especially ones where the input points are the result of computations involving many arithmetic operations.

\subsection{Bucket Statistics}
\label{sec:bucket-stats}

The same underlying implementation is used for the Pseudo-Pyramid Tree, Pyramid Tree and Bucket Hash Table. Only how the hashing function allocates points to buckets varies. Pyramid Tree's and Bucket Hash Table's hash functions have also been parallelised using SSE, which has made the difference between execution times of the individual hash functions of the three structures very small, allowing for a fair comparison of execution time.

\begin{table}
	\centering
	\makebox[\textwidth][c]{%
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			& & \multicolumn{3}{c|}{\textbf{Bucket Size Statistics}} \\
			\hline
			\textbf{Dataset} & \textbf{Time to Query (sec)} & \textbf{Average} & \textbf{Max} & \textbf{\#Buckets} \\
			\hline
			\textbf{Pseudo-Pyramid Tree} & & & & \\
			500,000 16D Random Points & 0.105091 & 1.0312  & 4 & TODO \\
			500,000 Astrophysics Points & 70.4391 & 3586.57 & 235260 & TODO \\
			500,000 Hurricane Isabel Points & 69.3996 & 17323.66 & 293949 & TODO \\
			435,544 3D Armadillo Mesh Points & 0.141219 & 19.1465 & 187 & TODO \\
			\hline
			\textbf{Pyramid Tree} & & & & \\
			500,000 16D Random Points & 0.177153 & 1.16285 & 7 & TODO \\
			500,000 Astrophysics Points & 60.0216 & 47820.89 & 143496 & TODO \\
			500,000 Hurricane Isabel Points & 69.4891 & 248977 & 497953 & TODO \\
			435,544 3D Armadillo Mesh Points & 0.13448 & 2.45933 & 1173 & TODO \\
			\hline
			\textbf{Bucket Hash Table} & & & & \\
			500,000 16D Random Points & 0.249102 & 1.01004 & 3 & TODO \\
			500,000 Astrophysics Points & 0.172516 & 1.01057 & 4 & TODO \\
			500,000 Hurricane Isabel Points & 0.222288 & 1.00987 & 3 & TODO \\
			435,544 3D Armadillo Mesh Points & 0.0811833 & 1.00857 & 3 & TODO \\
			\hline
		\end{tabular}
	}%
	\caption{Statistics on Bucket Size, Based on Dataset, of All Hash-Based Structures}
	\label{tab:perf2-bucket-stats}
\end{table}

Since the implementation details of the three structures are similar, the bucket size will be used to compare the structures. Table \ref{tab:perf2-bucket-stats} contains statistics on bucket size of all three hashing functions, using all real datasets and the 16D synthetic dataset.

From these results it is clear that the core factor which determines the performance of each structure is bucket size. The Pseudo-Pyramid Tree and Pyramid Tree have much larger buckets when storing the scientific datasets compared to when they store the synthetic dataset or armadillo mesh. With hurricane Isabel, one bucket in the Pyramid Tree is as large as 497,953, causing it degenerate to semi-sequential scan. When the average bucket size tends to one, all the structures can query all the points in the dataset in less than a half a second.

\subsection{Summary}

This iteration has explored different hashing functions and shown that bucket size is the core factor affecting the performance of hash-based index structures. All three hashing functions explored can be used to process the synthetic data and the 3D armadillo mesh in very little time, relative to Sequential Scan. Bucket Hash Table provides the best performance for the scientific datasets because it ensures the average bucket size tends to one, but is very susceptible to floating point error. The Pyramid Tree is less susceptible to such errors, but like the Pseudo-Pyramid Tree, degenerates to semi-sequential scan for the two scientific datasets.
