\chapter{Methodology}
\label{chap:methodology}
\centerline{\rule{149mm}{.02in}}
\vspace{2cm}

This section gives information on how the project will be tackled, describing the schedule (plus any changes that have made since its initial creation), tools and technologies that will be used throughout the project.

\section{Schedule}

TODO

\subsection{Phases}

TODO

TODO: gantt chart here???

\subsection{Milestones}

TODO

\subsection{Iteration Summary}

TODO: summary of each iteration and what was achieved/worked on in that iteration

\section{Technology}

TODO: fill this in

\subsection{Programming Language}

There exist many programming languages, all developed for specific purposes. Three programming languages have been considered for this project, with each having different properties. This section will describe these languaes, discuss their differences and state which will be used for this project and why.

\textbf{Python} is a high-level interpreted language built for general-purpose computing, which has a large standard library and many third-party libraries \cite{python}. \textbf{C++} is a middle-level, compiled systems programming language, combining low-level features (e.g. manual memory management) and high-level features (e.g. object-orientated programming) \cite{cpp}. C++ applications are compiled straight to the native machine's CPU instruction set, meaning less time is spent translating the code to instructions the hardware can directly execute. \textbf{Haskell} is a purely functional general-purpose programming language which compiles to a native executable, similar to C++ \cite{haskell}. Being purely functional means that applications written in Haskell have no mutable state. This is unlike Python and C++, whose applications have mutable state (since both languages are imperative).

The following aspects have been considered when deciding which language to use:
\begin{itemize}
	\item \textbf{Performance} -- how fast the final application can run. This is based on a number of factors, such as how many intermediary layers there are between the code and the physical machine. Higher-level languages require the computer to do more work, as it has to convert the application's instructions into CPU instructions that are executed directly on the processor.
	\item \textbf{Ease/Speed of Development} -- this encompasses multiple aspects of a language. In order to develop applications quickly, the language must have a mature tool set, large amounts of documentation and support, be easy to understand and require little boilerplate code (so less time is required to write the code). This makes it easier for the developer to write the application, increasing the speed it can be developed.
	\item \textbf{Portability} -- can an application written in a given language execute on many supported platforms or just one? Do multiple versions of the application have to be written or compiled for each platform?
\end{itemize}
 
Executing a Python statement often has more overhead than it would for C++ or Haskell. This is because Python is a high-level, interpreted language, where statements are compiled into bytecode at runtime which is then run on a virtual machine (which translates the bytecode to native assembly instructions). Therefore, many Python applications run slower than C++ or Haskell, which pre-compile applications to native CPU instructions. However, due to the large amount of experience I have in Python, its large standard library and the higher number of abstractions, Python would most likely result in the fastest development time. 

Haskell's pure functional nature makes it fundamentally different than most popular languages and I also have little experience writing programs in a purely functional style. It is predicted this will increase the time it takes to develop the structures, as there is more overhead in learning about functional programming. However, if it is decided that the index structures will be parallelised, then the lack of state and built-in support for parallelisation \cite{parallel-haskell} would make Haskell a powerful choice.

Despite speed of development being important, the primary aim of the project is the acceleration of search. Haskell can be used to write highly optimised code, especially when implementing parallel algorithms to utilise multiple CPU cores. C++ gives the developer more control over the computation and management of memory, allowing for low-level performance tuning. The trade-off for this control is the added complexity of the language \cite{cpp-hard}, leading to an increased amount of code to write and the decrease in development speed that follows. However, C++ is also a mature language with a large set of tools and resources to aid development. Additionally, I have significantly more experience developing in C++ than Haskell. It is unlikely I would be able to produce well-written, optimised Haskell code to fine-tune performance in such a short time. Therefore, C++ has been chosen as the language to use for developing the initial implementation of the index structure.

\subsection{Development Tools}
\label{sec:development-tools}

Various tools will be used throughout the development of the structures. \textbf{CMake} \cite{cmake} will be used to automate the building of the written C++ programs. Using a build automation tool means a Makefile can be automatically generated by writing simpler, higher-level and cross-platform scripts, meaning less time is spent focused on build scripts.

Unit tests will be written to ensure the implemented index structures and all the associated algorithms are functioning correctly. A unit test ensures a single unit of the code is exhibiting the desired behaviour. A test is performed in isolation, preventing other code in the application from being executed so it does interfere with the test's results \cite{automated-defect-prevention}. A unit test framework is a suite of code and tools that make writing unit tests easier and faster \cite{unit-test-frameworks}. C++ has many unit testing frameworks, such as CppUnit \cite{cppunit}, NullUnit \cite{nullunit} and Google Test \cite{google-test}. \textbf{Google Test} has been chosen for this project because of the wide feature set and the comprehensive documentation available (at \cite{google-test}).

Version control is a way of tracking changes to textual documents, as well as \textit{who} made the changes \cite{pragmatic-version-control}. A VCS (version control system) will be used to track the changes of the implementation's source code, tests and built automation scripts. Despite this project only having a single developer, there are multiple reasons it has been decided to manage the source code using a VCS. Firstly, it allows the developer to easily keep a log of the changes made to any file and \textit{why} those changes were made. One can also revert source files to prior versions if bugs are found or previously deleted code is required again, Finally, using a VCS means there will be multiple copies of the source code, which can be used as backups to mitigate the damage caused by data loss. The distributed VCS \textbf{Git} \cite{git} will be used for this project, due to the developer having prior experience in the tool, meaning less time is taken from implementation to learn a new tool.

\subsection{Performance Analysis Tools}

In addition to considering the theoretical performance of the chosen algorithms, the implementations' performance will be tested using profiling. Profiling is a way of measuring the performance of a program or system \cite{efficient-cpp} and is used to provide insight into which parts of the code take the longest or use the most memory (i.e. where the performance bottlenecks are). A profiler is a tool which measures some performance metrics of a program. Since the core goal of this project is accelerating multi-dimensional search, being able to measure the performance of the implementations and identify where the performance bottlenecks are is incredibly useful.

To perform the evaluation (see Section \ref{sec:evaluation}), a profiler that can measure heap usage and cache misses will be used. One which can produce call graphs with timings, that measure the flow of execution and how much time is spent in each function, is also desired. Using a profiler that produces little slow down is useful, but not critical, since timings could be scaled to take the extra overhead into consideration. 

Many profilers were considered when choosing one for this project. Intel VTune \cite{intel-vtune} is feature-rich profiler for Intel-based machines, but it is proprietary, so it will not be used for this project. Valgrind \cite{valgrind}, Profiny \cite{profiny} and gperftools \cite{gperftools} are freely available profilers. Out of these, only Valgrind can measure both heap usage and cache misses. gperftools can generate a visualisation of program flow and where the program spends most of its time via a call graph image, with just a single command. gperftools provides a similar feature for heap profiling as well. Therefore, both Valgrind and gperftools will be used for profiling.

\subsection{SSE Optimisation}
\label{sec:sse}

SIMD stands for Single Instruction, Multiple Data and was defined by Flynn as a classification of parallel computing \cite{flynns-taxonomy}. In SIMD, a single instruction is used to operation on multiple data items at the same time. If $p$ is the maximum number of data items that can be operated on in parallel at a time, then in an ideal scenario it is possible increase the speed of a computation by $p$ times. However, it is only suitable for computations where the same operation can be applied to multiple data items independantly, where the order in which those operations complete does not affect the final output.

Streaming SIMD Extensions, or SSE, is a specification of an instruction set that performs SIMD operations for the widely used x86 CPU architecture \cite{sse}, which is the architecture used for the development and test environment of this project. SSE, or specifically SSE2, since that's the latest version supported by the development environment, may be used to accelerate parts of the implementations, which are used frequently, that perform a collection of independant computations.