\section{Challenges in Multi-dimensional Search}
\label{sec:challenges}

A significant number of index structures were developed in the 1970s and 1980s, which were shown to provide good performance through empirical usage. Despite the efficiencies gained from these foundational structures, there are still numerous challenges to overcome. As such, work on developing new index structures has continued throughout the past twenty years. With increasing amounts of data being produced and the high number of dimensions said data may contain causes some of these existing structures to perform extremely poorly.

Four core challenges repeatedly discussed in the literature have been identified. Many attempts have been made to mitigate the effects of these challenges through new or modified index structures. This section will discuss these challenges, the impact they have on multi-dimensional search efficiency and \textit{why} they have these impacts.

\subsubsection{Curse of Dimensionality}
\label{sec:curse-of-dimensionality}

The curse of dimensionality is a term used to refer to the issues that occur when data with large numbers of dimensions is processed \cite{curse-of-dimensionality}. Spatial partitioning of high-dimensional feature vectors or geometric points becomes difficult because large regions of the data space are empty. This causes many index structures to contain many empty or near-empty nodes/regions, resulting in a large amount of memory overhead that does not boost performance (e.g. many regions in octrees with high $d$ are completely empty but exist anyway to due to uniformly sized regions). This is especially problematic when many of the dimensions might not tell you anything relevant about the data \cite{irrelevant-dimension}, causing large amounts of memory and computation to be wasted. Relating this back to multi-dimensional search specifically, Indyk and Motwani in \cite{knn-curse-of-dimensionality} discuss how efficient \textbf{nearest neighbour search} with a small number of dimensions is ``well-solved", but with a high number of dimensions the problem is much more challenging.

The majority of index structures discussed in the literature are hierarchical. Due to data sparseness caused by a higher number of dimensions, these structures often become very large and have many empty or near-empty nodes. Additionally, balanced splits and overlapping node regions cause \textit{most} of the nodes to pass boundary/intersection tests when a higher number of dimensions are used. This means the execution time of queries and dynamic operations tend to $O(n)$. In other words, the structures are no faster than a brute force sequential scan through a list, just with more memory overhead. Weber, Schek and Blott in \cite{va-file} show how hierarchical index structures which perform data space partitioning tend to perform poorly when 10 or more dimensions is used. They even show that there is no index structure ``based on clustering or partitioning which does not degenerate to a sequential scan if the dimensionality exceeds a certain threshold."

Therefore, for high $d$, sequential scan or methods based on linear data structures (e.g. lists) often provide better performance than methods which recursively decompose space into tree structures \cite{md-structures-samet}. This leads to the conclusion that the curse of dimensionality removes many of the benefits gained from using traditional index structures when high dimensional data is used.

\subsubsection{Variation in Data}

When developing an index structure, one does not know the nature of the input data in advance. Therefore, it is unwise to make too many assumptions when evaluating the efficiency of your structure with test data. Some datasets may be uniform, some may have noticeable skew and others may have a large number of clusters with complex shapes in data space. Different index structures are more efficient with different kinds of data. For example, quad/octrees become inefficient with non-uniform and skewed data and kd-tree variants often have poor utilisation of nodes \cite{bkd-tree} (i.e. few nodes actually have many data points).

For some applications, where the input data's structure may be known in advance, this is not an issue, since an index structure known to perform well with that type of data can be chosen. If the nature of the data is not known in advance however, developing a generic index structure that provides efficient queries (and dynamic operations) for \textit{all} kinds of data is challenging.

Wang et al. evaluated the PK-tree using uniformly distributed and clustered synthetic data as well as large real data sets \cite{pk-tree}. This allows for a comprehensive evaluation on how well the PK-tree performs on different kinds of data. Berchtold et al. also used a combination of carefully generated synthetic data and real data sets when evaluating the performance of Pyramid-trees \cite{pyramid-tree}. 
Comprehensively evaluating an index structure's performance therefore requires a varied collection of data, which includes:
\begin{itemize}
	\item Uniformly distributed data
	\item Non-uniform data (skewed and clustered)
	\item Large real world data sets
\end{itemize}	
Both generating/finding these types of data and ensuring an index structure performs well on most inputs have been found to be significant challenges.

\subsubsection{Dynamically Constructing Structures}

In order to provide fast queries, it is common to impose invariants that maintain a balanced structure. An invariant is a condition or property of the index structure that \textbf{must hold} to ensure good performance of queries. This is simple for static index structures, since the structure is built once and never changes. When inserting, deleting or updating points with dynamic index structures, maintaining invariants can be non-trivial. A simple example of a dynamic index structure is the red-black tree, which rotate various components of the tree repeatedly to fix broken invariants when a new point is inserted or a point is deleted \cite{introduction-to-algorithms}.

These invariants can result in \texttt{insert}, \texttt{update} and \texttt{delete} operations being slow. Creating index structures that provide fast queries without requiring invariants that are expensive to maintain can be difficult. The balance between dynamic construction performance and query performance depends on the application. If the index structure must be updated often, then prioritising fast construction over fast queries may be appropriate.

\subsubsection{Memory Access and Paging}
\label{sec:paging}

Different index structures have different storage requirements. It is important to consider this when choosing/developing an index structure. Structures whose size in memory grows quickly as the amount of data increases may not be able to fit entirely in main memory, meaning some of it will have to be \textbf{paged} in secondary memory (e.g. hard disk). This drastically affects performance, as it is much slower to access the hard disk than accessing main memory. This issue is amplified for high-dimensional data, as more memory is consumed per data item.

Some structures require little additional data to maintain the structure, such as B${}^{+}$-trees, pyramid trees and splay quadtrees \cite{ubiquitous-btree, pyramid-tree, splay-quadtree}. These have \textbf{low memory overhead}, meaning they scale well to large data sets. Other structures, such as PK-trees, require more data because more complex book-keeping is required to enable fast dynamic operations and queries \cite{pk-tree}. If a problem deals with massive amounts of data points, it may be worth using slower index structures with less overhead, in order to minimise the amount of memory used by the index structure. If the structure grows so large that it cannot fit in main memory, then \textit{any} index structure's performance will take a drastic hit because data has to be retrieved from secondary memory frequently.

For large datasets (terabytes in size, say), it is inevitable that some of the index structure will be stored in secondary memory. This core bottleneck has been identified and researchers have constructed index structures, called bucket methods, that store data items in ways that minimise I/O operations \cite{md-structures-samet}. Some structures have parameters that can be tweaked to find optimal paging with the target hardware or dataset, such as the PK-tree and pyramid tree \cite{pyramid-tree, pk-tree}.

To conclude, if the index structure is used with large, complex data items then some of it may have to be stored in secondary storage, especially if there are many data points. Designing an index structure to facilitate quick access to secondary memory and reduce query times has been the focus of many researchers in the field  \cite{rsr-tree, rs-tree}.
