\chapter{Evaluation Methodology}
\label{chap:evaluation-outline}
\centerline{\rule{149mm}{.02in}}
\vspace{2cm}

One of the project's deliverables is an evaluation of the implemented index structure(s). Multiple evaluations will be performed to guide what needs to be achieved in each iteration of development and determine how efficient the final implementations at the end of the project are. As such, evaluation forms a large part of the project. This section details what measurements will be used to evaluate implementation performance, what the baselines are (i.e. what the implementations will be compared to) and what data will be used for the evaluation.

\section{Performance Measures}
\label{sec:performance-measures}

A variety of measurements have been used to evaluate the performance of index structures. Which measurements to use depend on the focus of the research and what problem the specific structure is trying to solve. Section \ref{sec:measuring-efficiency} discusses the ways index structures are commonly evaluated in the literature. This project is aimed towards accelerating index structures both algorithmically and by optimising the implementation itself. The performance analysis tools discussed in Section \ref{sec:development-tools} will be used to produce some of the chosen evaluation measures, which are:

\begin{itemize}
	\item \textbf{Total Execution Time} -- total time it took an operation to execute
	\item \textbf{Cache Miss Rate} -- rate of cache misses over all cache accesses ($\frac{\text{\# cache misses}}{\text{total cache accesses}}$)
	\item \textbf{Peak Heap Memory} -- maximum amount of heap memory used at once by the simulation 
	\item \textbf{Speedup Factor} -- increase in speed when compared to another index structure (e.g. evaluation baselines). This is given by $\frac{t_1}{t_2}$, where $t_1$ and $t_2$ are the running times of the two index structures being compared.
\end{itemize}

One assumption being made about the target data is that it is dynamic, meaning points may be inserted or deleted at any time. For query performance, the evaluation will focus on the performance of point queries. Therefore, index structures will be evaluated using the performance of their \textbf{insert}, \textbf{delete} and \textbf{point query} operations.

\section{Timing Operations}
\label{sec:timing-operations}

Amortised analysis, introduced by Tarjan in 1985, is used to analyse the performance of algorithms over a sequence of operations, rather than an individual one \cite{amortised-analysis}. This type of analysis is typically used for algorithms where some operations take longer than others in order to make future operations quicker. The amortised time complexity of an algorithm describes the \textit{average} runtime of an operation over a sequent of operations. Some index structures, particularly self-adjusting ones, have used this to bound the running time of their operations, such as the Splay Tree, Quadtreap and Splay Quadtree (see Section \ref{sec:history-sensitive-structures}).

This project will use a similar concept for measuring the execution time of the structures. Instead of measuring the time of a single operation, the times of many thousands of operations will be measured. This is to get an idea on what the runtime of an operation is on average, over a sequence of operations. This prevents single, slow operations from causing the execution time to appear worse, or short operations making the performance appear better. A typical performance test will the \textbf{Insert-Query-Delete} operation list, which has the following steps:
\begin{enumerate}
	\item \textbf{Initial Build} -- incrementally build structure by adding each point in test dataset one-by-one
	\item \textbf{Query Points} -- query each point in the dataset
	\item \textbf{Delete Points} -- delete each point in the dataset from the structure until it is empty
\end{enumerate}
Using this process allows the runtime of large numbers of \texttt{insert}, \texttt{delete} and point query operations to be measured. Any performance tests of sequences of operations will be performed five times, and the recorded time will be the \textit{average} of those five times.

\section{Datasets}
\label{sec:datasets}

This section defines the datasets that will be used for the evaluation. Both synthetic, artificially generated data and real datasets will be used to evaluate the performance of the index structures.

\subsection{Synthetic Datasets}

Three types of randomly generated \textit{synthetic} datasets will be generated, which are uniformly distributed, skewed and clustered. All generated points are in $[0,1]^d$. Multiple instances of these datasets will be used, with varying numbers of dimensions. Using such datasets was inspired by Wang et al. and Berchtold et al., who used similar datasets to evaluate the performance of the PK-tree and pyramid tree respectively \cite{pk-tree, pyramid-tree}. The reason for testing the index structures with these datasets is that the variety of point distributions and number of dimensions should tease out which data the implementations struggle with and which they work well at.

The synthetic data was generated randomly using the \texttt{boost::random} library and the Mersenne twister pseudorandom number generator (PRNG) \cite{mersenne-twister}.  The Mersenne twister passes ``stringent statistical tests" of randomness, including diehard \cite{mersenne-twister}. The \texttt{boost::random} function was chosen over using the C standard library function \texttt{rand()} because the latter is platform-dependent. Therefore, one cannot assume the underlying PRNG used by \texttt{rand()} passes these statistical tests.

Figure \ref{fig:synthetic-data} shows 2D randomly generated points using uniform, skewed and clustered distributions. Skewed data was generated by applying a power to the number. Let $p$ be a generated point. Since $0 \leq p_i \leq 1$ for all $i \in \lbrace 1, ..., d \rbrace$, this means $p_i^e \leq p_i$ for all $e \geq 1$. This makes smaller values more likely, generating a skewed distribution of points. $e = 1.5$ was used for all skewed datasets generated. The clustered datasets contain two clusters of points, each contained within the hypercubes $[0,0.5]^d$ and $[0.7,0.8]^d$. There is zero probability that points will appear outside of those cubes.

\begin{figure}
	\begin{center}
		\begin{subfloat}[Uniform Distribution\label{fig:uniform-distribution}]{%
			\includegraphics[scale=0.25]{figures/uniform_distribution.pdf}
		}
		\end{subfloat}~
		\begin{subfloat}[Skewed Distribution\label{fig:skewed-distribution}]{%
			\includegraphics[scale=0.25]{figures/skewed_distribution.pdf}
		}
		\end{subfloat}~
		\begin{subfloat}[Clustered Distribution\label{fig:clustered-distribution}] {%
			\includegraphics[scale=0.25]{figures/clustered_distribution.pdf}
		}
		\end{subfloat}
	\end{center}

	\caption{Three Types of Synthetic Dataset Used For Evaluation}
	\label{fig:synthetic-data}
\end{figure}

Multiple synthetic datasets with 10,000 points have been generated for each distribution, with varying numbers of dimensions. Tests using these datasets will measure the \textit{total} runtime of the operations. The structures will also be measured against data of varying size. Randomly generated points with a uniform probability distribution have been generated, which will be sampled to construct each dataset. This dataset has 500,000 points and 16 dimensions, which was chosen to match the tests performed by Berchtold et al. for their Pyramid Tree in \cite{pyramid-tree}. The total runtime will also be used to measure performance against these datasets, despite the number of operations varying. Relationships between structure performance and dataset size can be still be gleaned using this measurement (e.g. if runtime increases linearly with dataset size, we can conclude that an implementation roughly has $O(n)$ performance). The Insert-Query-Delete operation list will be used for all of these datasets. 

\subsection{Real Datasets}

Three real datasets will also be used for the evaluation. The first is the result of an \textbf{astrophysics turbulence simulation}, where a $600 \times 248 \times 248$ regular mesh was used to simulate ``three-dimensional radiation hydrodynamical calculations of ionization front instabilities" \cite{astrophysics-dataset}. 200 timesteps were recorded, each being approximately 126 to 128 years apart \cite{astrophysics-dataset}. For each evaluation, only \textbf{one timestep} of the simulation will be used at a time. Since some of the timesteps have different distributions, multiple tests with different timesteps may be performed. Each point of the mesh has ten scalar fields, which include particle density, temperature and eight chemical species. Figure \ref{fig:astrophysics} shows an example of a visualisation generated using a 2D Z-slice of the data, where $z = 124$ and timestep $t = 30$.

The second real dataset contains 13 scalar fields for every point on a $500 \times 100 \times 100$ mesh, over 48 timesteps. It is the result of a simulation of hurricane Isabel from 2003 \cite{hurricane-isabel-dataset} (see Figure \ref{fig:hurricane-isabel} for a visualisation of this dataset).

The final real dataset is a point cloud of a 3D armadillo mesh from The Stanford 3D Scanning Repository \cite{armadillo-mesh}, which contains 435,544 points. This mesh has been used by other researchers to evaluate index structure performance \cite{kd-tree-gpu, accelerating-kdtree-nn}.

\begin{figure}
		\begin{center}
			\begin{subfloat}[Z-Slice of Timestep 30 of Astrophysics Dataset, Displayed Using Blackbody Radiation Spectrum \cite{astrophysics-dataset}\label{fig:astrophysics}]{%
				\includegraphics[scale=0.3]{figures/ionisation_front_instabilities.pdf}
			}
			\end{subfloat}~
			\begin{subfloat}[Hurricane Isabel Simulation\label{fig:hurricane-isabel}]{%
				\includegraphics[scale=0.175]{figures/hurricane_isabel.pdf}
			}
			\end{subfloat}~
			\begin{subfloat}[3D Armadillo Mesh\label{fig:armadillo-mesh}] {%
				\includegraphics[scale=1.5]{figures/armadillo.pdf}
			}
			\end{subfloat}			  
		\end{center}

		\caption{Three Real Datasets Used For Evaluation}
		\label{fig:real-data}
\end{figure}

The astrophysics and hurricane datasets were chosen because they are large and have a high number of dimensions (10 and 13 respectively). The core focus of the project is high-dimensional data, so this data will provide a true test of how well the implementations perform with real instances of such data. Additionally, both datasets are used for scientific visualisation and contain continuous values. As discussed in Section \ref{sec:core-assumptions}, this type of data is what the project is targeting.

For intermediate evaluations, it was decided to use smaller instances of these datasets for performance tests. Sampling a sub-region of the grid or uniformly sampling to create a dataset with a smaller resolution may introduce \textit{bias} in the data, which affects the evaluation. Therefore, smaller instances of the dataset are generated by randomly sampling $n$ points from the original dataset, discarding and picking other points whenever a point that has already been sampled was chosen. The law of large numbers \cite{large-sample-theory} shows that, as the number of samples increase, the sampled dataset starts to become more representative of whole dataset, making this method statistically fairer.

$500,000$ points will be sampled from timestep 100 and 24 of the astrophysics turbulence and hurricane Isabel simulations respectively. These timesteps were chosen because there is a greater amount of \textit{``activity"} in the data. That is, the phenomena being modelled starts develop as more time passes, meaning there is a greater variety of points and the distributions. The timesteps at the start of such simulations tend to be sparse and uniform. Both of these timesteps still contain some amount of sparsity however, resulting in dense regions of activity surrounded by relatively sparse, uniform regions. This type of data is common in scientific computation, so these datasets should act as a good test on how well the index structures can handle data in such a domain.

In the hurricane Isabel dataset, regions of the space which contained ground were not recorded. The points inside those regions have values of 1.0e+35 to mark they have not been recorded. Since these points given no useful information about the simulation and widely skew the dataset (due to the large magnitude of 1.0e+35), these have been removed from the dataset.

The sampled points will be used as the real datasets for the evaluation, using the Insert-Query-Delete operations list to time the structures. Again, the total runtime of operations will be measured Table \ref{tab:operation-lists} shows the operation lists that will be used for evaluation, which datasets the points will be pulled from, the different values the varying parameter (dimension or dataset size) will have and what performance measure will be used for an operation. All 435,544 points of the 3D armadillo mesh will be used.

\begin{table}
	\centering
	\makebox[\textwidth][c]{%
	\begin{tabular}{|p{4.5cm}|p{3.5cm}|p{2cm}|p{3cm}|}
		\hline
		\textbf{Dataset(s)}
			& \leftspecialcell{\textbf{Fixed} \\ \textbf{Parameter(s)}}
			& \leftspecialcell{\textbf{Varying} \\ \textbf{Parameter}}
			& \textbf{Parameter Values}  \\
		\hline
		Uniform, Skewed, Clustered & 10,000 Points & Dimension & \leftspecialcell{1, 2, 3, 5, 10, \\ 50, 100, 200} \\
		\hline
		16D Uniform & 16 dimensions & Input Size & \leftspecialcell{10,000, 100,000, \\ 500,000} \\
		\hline
		Astrophysics ($t = 100$) & \leftspecialcell{10 dimensions, \\500,000 points} & - & - \\
		\hline
		Hurricane Isabel ($t = 24$)& \leftspecialcell{13 dimensions, \\500,000 points} & - & - \\
		\hline
		Armadillo Mesh & \leftspecialcell{3 dimensions, \\435,544 points} & - & -\\
		\hline
	\end{tabular}}%
	\caption{Operation Lists Used for Main Evaluation}
	\label{tab:operation-lists}
\end{table}

\section{Baselines}
\label{sec:baselines}

There is little use in measuring the performance of the implemented index structures for the purposes of evaluation if there is nothing to compare the results to. As such, two baseline index structure implementations will be developed -- \textbf{sequential scan} and the \textbf{PR octree} (see Section \ref{sec:recursive-partition-structures}). The reason for choosing sequential scan as a baseline is that it's the na\"{i}ve brute-force approach to performing search. The PR octree is the basis of many index structures, both old and new. Thus, the structure is well-known in the field and makes a suitable baseline.

These baselines have been developed in C++ to match the technology used to develop the initially implemented index structure.

\section{Environment}

The machines in the University of Leeds' School of Computing laboratory have been used to develop the index structures and measure their performance. Table \ref{tab:system-specifications} lists the hardware, operating system and tool versions these machines use.

\begin{table}
	\centering
	\begin{tabular}{|r|l|}
		\hline
		\textbf{CPU} & Intel(R) Xeon(R) CPU E3-1225 V2 @ 3.20 GHz (four cores) \\
		\hline
		\textbf{Main Memory (RAM)} & 15.5 GB \\
		\hline
		\textbf{Operating System} & Linux CentOS 6.5 (Final) \\
		\hline
		\textbf{Linux Kernel Version} & 2.6.32-431.3.1.el6.x86\_64 \\
		\hline
		\textbf{C++ Compiler} & GCC 4.4.7 (Released 13/03/2013) \\
		\hline
		\textbf{Boost Library Version} & 1.41.0 \\
		\hline
	\end{tabular}
	\caption{Hardware, Operaring System and Tool Versions used for Development and Performance Testing}
	\label{tab:system-specifications}
\end{table}

\subsection{Measuring Execution Time}

There are various ways of measuring the execution time of an application. Individual index structure operations are typically much lower than a second, sometimes even less than a millisecond. A high level of precision and accuracy is required because even millisecond savings are important when optimising index structure operations are required.

C/C++ come with two standard timing mechanisms. \texttt{time()}\footnote{Online documentation for \texttt{time()}: \url{http://www.cplusplus.com/reference/ctime/time/}} returns the number of seconds since the Unix epoch (00:00, 01/01/1970 UTC). Since it returns seconds, it is not precise enough for this project. \texttt{clock()}\footnote{Online documentation for \texttt{clock()}: \url{http://www.cplusplus.com/reference/ctime/clock/}}  measures the number of CPU ticks since some epoch (typically the launch of the application). The amount of real time (i.e. milliseconds, seconds, etc.) a CPU tick takes is constant, but depends on the hardware.

\texttt{clock\_gettime()}\footnote{Online documentation for \texttt{clock\_gettime()}: \url{http://linux.die.net/man/3/clock_gettime}} returns a time with nanosecond precision, but is only available on specific systems (e.g. Linux). Since the operating system used for the evaluation framework is Linux and this level of precision is desired, \texttt{clock\_gettime()} has been used for timing all operations.