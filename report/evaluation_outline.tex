\chapter{Outline to Evaluation Approach}
\label{chap:evaluation-outline}
\centerline{\rule{149mm}{.02in}}
\vspace{2cm}

One of the project's deliverables is an evaluation of the implemented index structure(s). Multiple evaluations will be performed to guide what needs to be achieved in each iteration of development and determine how efficient the final implementations at the end of the project are. As such, evaluation forms a large part of the project. This section details what measurements will be used to evaluate implementation performance, what the baselines are (i.e. what the implementations will be compared to) and what data will be used for the evaluation.

\section{Measures}

A variety of measurements have been used to evaluate the performance of index structures. Which measurements to use depend on the focus of the research and what problem the specific structure is trying to solve. Section \ref{sec:measuring-efficiency} discusses the ways index structures are commonly evaluated in the literature. This project is aimed towards accelerating index structures both algorithmically and by optimising the implementation itself. The performance analysis tools discussed in Section \ref{sec:development-tools} will be used to produce some of the chosen evaluation measures, which are:

\begin{itemize}
	\item \textbf{Total Execution Time} -- total time it took an operation to execute
	\item \textbf{Cache Hit Rate} -- rate of cache hits to misses ($\frac{\text{\# cache hits}}{\text{\# cache misses}}$)
	\item \textbf{Total Heap Memory} -- amount of heap memory used over the entire simulation
	\item \textbf{Peak Heap Memory} -- maximum amount of heap memory used at once by the simulation 
	\item \textbf{Total Memory} -- total amount of stack and heap memory used over the entire simulation
	\item \textbf{\# Heap Allocations} -- the amount of system calls made to allocate memory on the heap
	\item \textbf{CPU/IO Ratio} -- the ratio between time spent on computation and time spent on I/O operations (e.g. cache reads, heap allocations, etc.)
	\item \textbf{Speedup Factor} -- increase in speed when compared to another index structure (e.g. evaluation baselines). This is given by $\frac{t_1}{t_2}$, where $t_1$ and $t_2$ are the running times of the two index structures being compared.
\end{itemize}

One assumption being made about the target data is that it is dynamic, meaning points may be inserted or deleted at any time. For query performance, the evaluation will focus on the performance of point queries. Therefore, index structures will be evaluated using the performance of their \textbf{insert}, \textbf{delete} and \textbf{point query} operations.

\section{Amortisation of Tests}

TODO: explain what amortmisation is

TODO: state that it has been used for theoretical analysis of algorithms (amortised analysis), such as the splay quadtree, quadtreap and Splay tree (Robert Tarjan was first one to use such a thing, or one of the first???)

This project will use this concept for evaluation by measuring the \textit{average} runtime of each operation, over many thousands of such operations. A typical performance test therefore has the following steps:
\begin{enumerate}
	\item \textbf{Initial Build} -- incrementally build structure by adding each point in test dataset one-by-one
	\item \textbf{Query Points} -- query each point in the dataset, which should not be in the structure
	\item \textbf{Delete Points} -- delete each point in the dataset from the structure until it is empty
\end{enumerate}
Using this process allows the \textbf{average} performance of large numbers of \texttt{insert}, \texttt{delete} and point query operations to be computed. 

By interleaving \texttt{insert} and \texttt{delete} operations with a number of point queries, more insight can be gained into how well the implementations perform in real applications that use dynamic data structures, where the pattern of operations is less straightforward. The performance of the implementations on interleaved operations may be different from the test process described previously due to cache coherency. The contents of the cache may change frequently due to different operations being performed consecutively, potentially causing a greater number of cache misses.

TODO: mention trace of Joint Contour Net behaviour that will be used to see how the index structures perform with a real application -- AND TO SEE HOW INTERLEAVED OPERATIONS 

\section{Data}

Both synthetic, artificially generated data and a real dataset will be used for the evaluation. Three types of \textbf{synthetic} datasets will be generated -- uniform distributed, skewed and clustered. Multiple instances of these datasets will be used, with varying numbers of dimensions (e.g. 1, 2, 3, 5, 10, 50, 100). Using such datasets was inspired by Wang et al. and Berchtold et al., who used similar datasets to evaluate the performance of the PK-tree and pyramid tree respectively \cite{pk-tree, pyramid-tree}. The reason for testing the index structures with these datasets is that the variety of point distributions and number of dimensions should tease out which data the implementations struggle with and which they work well at.

TODO: how synthetic data was generated

The real dataset being used for the evaluation is the result of an \textbf{astrophysics turbulence simulation}, where a $600 \times 248 \times 248$ regular mesh was used to simulate ``three-dimensional radiation hydrodynamical calculations of ionization front instabilities" \cite{astrophysics-dataset}. Each point of the mesh has ten scalar fields, which include particle density, temperature and eight chemical species. 200 timesteps were recorded, each being approximately 126 to 128 years apart \cite{astrophysics-dataset}. For each evaluation, only \textbf{one timestep} of the simulation will be used at a time. Since some of the timesteps have different distributions, multiple tests with different timesteps may be performed.

The astrophysics was chosen because it is large (almost 37 million points) and has a high number of dimensions (10). The core focus of the project is high-dimensional data, so this data will provide a true test of how well the implementations perform with real instances of such data. Various resolutions of the dataset may be used for testing to reduce the amount of data being tested at any one point. Datasets with smaller resolutions will be constructed by \textbf{uniformly sampling} the points of the original simulation's mesh.

TODO: how the astrophysics dataset was uniformly sampled

\section{Baselines}

There is little use in measuring the performance of the implemented index structures for the purposes of evaluation if there is nothing to compare the results to. As such, two baseline index structure implementations will be developed -- \textbf{sequential scan} and the \textbf{PR octree} (see Section \ref{sec:recursive-partition-structures}). The reason for choosing sequential scan as a baseline is that it's the na\"{i}ve brute-force approach to performing search. The PR octree is the basis of many index structures, both old and new. Thus, the structure is well-known in the field and makes a suitable baseline.

These baselines will be developed in C++ to match the technology used to develop the initially implemented index structure.
