\chapter{Evaluation Methodology}
\label{chap:evaluation-outline}
\vspace{-0.75cm}
\centerline{\rule{149mm}{.02in}}
\vspace{0.75cm}

The heart of this project is determine which index structure performs the best for high-dimensional, scientific datasets. Several evaluations have been performed for this purpose. This section details the measurements, baselines and data used for these evaluations.

\section{Performance Measures}
\label{sec:performance-measures}

How to evaluate index structures depends on the focus of the research and what the structures are being used for. Section \ref{sec:measuring-efficiency} discusses the ways index structures are commonly evaluated in the literature.

The target data is assumed to be dynamic (Section \ref{sec:core-assumptions}), meaning points may be inserted or deleted at any time. The project is concerned with point query performance, not range or nearest neighbour queries. Therefore, index structures are evaluated against the performance of their \textbf{insert}, \textbf{delete} and \textbf{point query} operations. For each of these operations the \textbf{total execution time} is measured.

The performance of index structures can be affected by their implementations. Therefore, additional measurements that will be computed to take implement factors into consideration. The first is \textbf{cache miss rate}, which is the rate of cache misses over all CPU reads. A cache miss occurs when the CPU has to fetch data from main memory, which introduces overhead. This overhead can add up and significantly impact the performance of applications.

The second measurement is \textbf{memory overhead}, which measures the extra memory used by an index structure to store a dataset. A structure has a memory overhead of 1 if it does not use any additional memory. The tools discussed in Section \ref{sec:development-tools} are used to produce some of these measures.

\section{Timing Operations}
\label{sec:timing-operations}

Amortised analysis, introduced by Tarjan in 1985, is used to analyse the performance of algorithms over a sequence of operations, rather than an individual one \cite{amortised-analysis}. This type of analysis is typically used for algorithms where some operations take longer than others in order to make future operations quicker. The amortised time complexity of an algorithm describes the \textit{average} runtime of an operation over a sequence of operations. Some index structures have used this to bound the running time of their operations, such as the splay tree, quadtreap and splay quadtree (see Section \ref{sec:history-sensitive-structures}).

This project will use a similar concept for measuring the execution time of the structures. Instead of measuring the time of a single operation, the times of many thousands of operations will be measured. This is to get an idea on what the runtime of an operation is on average, over a sequence of operations. This prevents single, slow operations from causing the execution time to appear worse, or short operations making the performance appear better. Each performance test will invoke the \textbf{Insert-Query-Delete} operation list on each structure, which has the following steps:
\begin{enumerate}
	\item \textbf{Initial Build} -- incrementally build structure by adding each point in test dataset one-by-one
	\item \textbf{Query Points} -- query each point in the dataset
	\item \textbf{Delete Points} -- delete each point in the dataset from the structure until it is empty
\end{enumerate}
Using this process allows the runtime of large numbers of \texttt{insert}, \texttt{delete} and point query operations to be measured. Any performance tests of sequences of operations will be performed five times, and the recorded time will be the \textit{average} of those five times.

\section{Datasets}
\label{sec:datasets}

This section defines the datasets that will be used for the evaluation, which includes both synthetic, artificially generated data and real datasets.

\subsection{Synthetic Datasets}

A variety of point distributions and number of dimensions may tease out the types of data the structures are suited to and which they struggle with. This project uses three kinds of synthetic data, which have uniform, skewed and clustered distributions respectively. Figure \ref{fig:synthetic-data} shows 2D randomly generated points using uniform, skewed and clustered distributions. All generated points are in $[0,1]^d$.

Multiple instances of these datasets will be used, with varying numbers of dimensions. Using such datasets was inspired by Wang et al. and Berchtold et al., who used similar datasets to evaluate the performance of the PK-tree and pyramid tree respectively \cite{pk-tree, pyramid-tree}.

Tu ensure the synthetic data is stastically fair, the Mersenne twister pseudorandom number generator (PRNG) \cite{mersenne-twister} was used to generate the points.  The Mersenne twister passes ``stringent statistical tests" of randomness \cite{mersenne-twister}. The Boost.Random\footnote{\url{http://www.boost.org/doc/libs/1_48_0/doc/html/boost_random.html}} library implements this PRNG and was chosen over using the C standard library function \texttt{rand()} because the underlying PRNG used by the function is platform-dependent, giving no guarantees it is statistically fair.

Skewed data was generated by applying a power to the number. Let $p$ be a generated point. Since $0 \leq p_i \leq 1$ for all $i \in \lbrace 1, ..., d \rbrace$, this means $p_i^e \leq p_i$ for all $e \geq 1$. This makes smaller values more likely, generating a skewed distribution of points. $e = 1.5$ was used for all skewed datasets generated. The clustered datasets contain two clusters of points, each contained within the hypercubes $[0,0.5]^d$ and $[0.7,0.8]^d$. There is zero probability that points will appear outside of those cubes.

\begin{figure}
	\begin{center}
		\begin{subfloat}[Uniform Distribution\label{fig:uniform-distribution}]{%
			\includegraphics[scale=0.25]{figures/uniform_distribution.pdf}
		}
		\end{subfloat}~
		\begin{subfloat}[Skewed Distribution\label{fig:skewed-distribution}]{%
			\includegraphics[scale=0.25]{figures/skewed_distribution.pdf}
		}
		\end{subfloat}~
		\begin{subfloat}[Clustered Distribution\label{fig:clustered-distribution}] {%
			\includegraphics[scale=0.25]{figures/clustered_distribution.pdf}
		}
		\end{subfloat}
	\end{center}

	\caption{Three Types of Synthetic Dataset Used For Evaluation}
	\label{fig:synthetic-data}
\end{figure}

Multiple synthetic datasets with 10,000 points have been generated for each distribution, with varying numbers of dimensions. The structures will also be evaluated against datasets with varying sizes. A dataset containing 500,000 uniform distributed points has been sampled to construct multiple, smaller datasets. These dataset 16 dimensions to match the tests performed by Berchtold et al. for the Pyramid Tree in \cite{pyramid-tree}.

% TODO: have this??
%The total runtime will also be used to measure performance against these datasets, despite the number of operations varying. Relationships between structure performance and dataset size can be still be gleaned using this measurement (e.g. if runtime increases linearly with dataset size, we can conclude that an implementation roughly has $O(n)$ performance).

\subsection{Real Datasets}

Three real datasets will also be used for the evaluation. The first is the result of an \textbf{astrophysics turbulence simulation}, where a $600 \times 248 \times 248$ regular mesh was used to simulate ``three-dimensional radiation hydrodynamical calculations of ionization front instabilities" \cite{astrophysics-dataset}.  Each point of the mesh has ten scalar fields, which include particle density, temperature and eight chemical species. 200 timesteps were recorded, each being approximately 126 to 128 years apart \cite{astrophysics-dataset}. Figure \ref{fig:astrophysics} shows an example of a visualisation generated using a 2D Z-slice of the data at timestep 30, where $z = 124$.

A single timestep is used for evaluation, which is timestep 100. This timestep were chosen because there is a greater amount of \textit{``activity"} in the data. The phenomena being modelled starts develop as more time passes, meaning there is greater variance in points. The timesteps near the beginning of simulation are sparse and uniform. Timestep 100 will still contain some amount of sparsity, resulting in dense regions of activity surrounded by relatively sparse, uniform regions. Distributions like this are common in scientific computation, so this timestep should act as a good test on how well the index structures can handle scientific datasets.

The second real dataset contains 13 scalar fields for every point on a $500 \times 100 \times 100$ mesh, over 48 timesteps. It is the result of a simulation of hurricane Isabel from 2003 \cite{hurricane-isabel-dataset} (see Figure \ref{fig:hurricane-isabel} for a visualisation of this dataset). Timestep 24 of this simulation will be used because, again, the data is highly sparse with dense regions of activity. In the original simulation, nothing was recorded in regions of space that contained ground. The points inside those regions have values of 1.0e+35 to mark they have not been recorded. Since these points given no useful information about the simulation, they have been removed from the dataset.

\begin{figure}
		\begin{center}
			\begin{subfloat}[Z-Slice of Timestep 30 of Astrophysics Dataset, Displayed Using Blackbody Radiation Spectrum \cite{astrophysics-dataset}\label{fig:astrophysics}]{%
				\includegraphics[scale=0.3]{figures/ionisation_front_instabilities.pdf}
			}
			\end{subfloat}~
			\begin{subfloat}[Hurricane Isabel Simulation\label{fig:hurricane-isabel}]{%
				\includegraphics[scale=0.175]{figures/hurricane_isabel.pdf}
			}
			\end{subfloat}~
			\begin{subfloat}[3D Armadillo Mesh\label{fig:armadillo-mesh}] {%
				\includegraphics[scale=1.5]{figures/armadillo.pdf}
			}
			\end{subfloat}			  
		\end{center}

		\caption{Three Real Datasets Used For Evaluation}
		\label{fig:real-data}
\end{figure}

The astrophysics and hurricane datasets were chosen because they are large and have a high number of dimensions (10 and 13 respectively). The core focus of the project is high-dimensional scientific data, so this data will provide a true test of how well the implementations perform with real instances of such data. Additionally, both datasets are used for scientific visualisation and contain continuous values. As discussed in Section \ref{sec:core-assumptions}, this type of data is what the project is targeting.

Each of these datasets contains millions of points. It was decided that a smaller instance of these datasets would be used, sampling 500,000 points from both datasets. Sampling a sub-region of the grid or uniformly sampling can introduce \textit{bias} in the data, which affects the evaluation. Therefore, smaller instances of the dataset are generated by randomly sampling $n$ points from the original dataset, discarding and picking other points whenever a point that has already been sampled was chosen. The law of large numbers \cite{large-sample-theory} shows that, as the number of samples increase, the sampled dataset starts to become more representative of whole dataset, making this method statistically fairer.

The final real dataset is a point cloud of a 3D armadillo from The Stanford 3D Scanning Repository \cite{armadillo-mesh}, which contains 435,544 points that represent a geometric mesh. This mesh has been used by other researchers to evaluate index structure performance applied to 3D graphics \cite{kd-tree-gpu, accelerating-kdtree-nn}.

Table \ref{tab:operation-lists} summarises the evaluation datasets, their size and dimensionalioty and lists any parameters that will be varied.

\begin{table}
	\centering
	\makebox[\textwidth][c]{%
	\begin{tabular}{|p{4.5cm}|p{3.5cm}|p{2cm}|p{3cm}|}
		\hline
		\textbf{Dataset(s)}
			& \leftspecialcell{\textbf{Fixed} \\ \textbf{Parameter(s)}}
			& \leftspecialcell{\textbf{Varying} \\ \textbf{Parameter}}
			& \textbf{Parameter Values}  \\
		\hline
		Uniform, Skewed, Clustered & 10,000 Points & Dimension & \leftspecialcell{1, 2, 3, 5, 10, \\ 50, 100, 200} \\
		\hline
		16D Uniform & 16 dimensions & Input Size & \leftspecialcell{10,000, 100,000, \\ 500,000} \\
		\hline
		Astrophysics ($t = 100$) & \leftspecialcell{10 dimensions, \\500,000 points} & - & - \\
		\hline
		Hurricane Isabel ($t = 24$)& \leftspecialcell{13 dimensions, \\500,000 points} & - & - \\
		\hline
		Armadillo Mesh & \leftspecialcell{3 dimensions, \\435,544 points} & - & -\\
		\hline
	\end{tabular}}%
	\caption{Evaluation Datasets}
	\label{tab:operation-lists}
\end{table}

\section{Baselines}
\label{sec:baselines}

There is little use in measuring the performance of the implemented index structures if there is nothing to compare the results to. Two baseline index structure implementations have been implemented -- \textbf{sequential scan} and the \textbf{PR octree} (see Section \ref{sec:recursive-partition-structures}). The reason for choosing sequential scan as a baseline is that it's the na\"{i}ve brute-force approach to performing search. The PR octree is the basis of many index structures, both old and new. Both of these structures are well-known in the field, making them suitable baselines. Both structures have been implemented in C++ to match the technology used by the other index structures implemented in this project.

\section{Environment}

The machines in the University of Leeds' School of Computing laboratory have been used to develop the index structures and measure their performance. Table \ref{tab:system-specifications} lists the hardware, operating system and tool versions these machines use.

\begin{table}
	\centering
	\begin{tabular}{|r|l|}
		\hline
		\textbf{CPU} & Intel(R) Xeon(R) CPU E3-1225 V2 @ 3.20 GHz (four cores) \\
		\hline
		\textbf{Main Memory (RAM)} & 15.5 GB \\
		\hline
		\textbf{Operating System} & Linux CentOS 6.5 (Final) \\
		\hline
		\textbf{Linux Kernel Version} & 2.6.32-431.3.1.el6.x86\_64 \\
		\hline
		\textbf{C++ Compiler} & GCC 4.4.7 (Released 13/03/2013) \\
		\hline
		\textbf{Boost Library Version} & 1.41.0 \\
		\hline
	\end{tabular}
	\caption{Hardware, Operaring System and Tool Versions used for Development and Performance Testing}
	\label{tab:system-specifications}
\end{table}

\subsection{Measuring Execution Time}

Individual index structure operations are typically much lower than a second, sometimes even less than a millisecond. A high level of precision and accuracy is required because even millisecond savings are important when optimising index structures.

C/C++ comes with two standard timing mechanisms. \texttt{time()}\footnote{Online documentation for \texttt{time()}: \url{http://www.cplusplus.com/reference/ctime/time/}} returns the number of seconds since the Unix epoch (00:00, 01/01/1970 UTC). Since it returns seconds, it is not precise enough for this project. \texttt{clock()}\footnote{Online documentation for \texttt{clock()}: \url{http://www.cplusplus.com/reference/ctime/clock/}}  measures the number of CPU ticks since some epoch (typically the launch of the application). The amount of real time (i.e. milliseconds, seconds, etc.) a CPU tick takes is constant, but depends on the hardware.

\texttt{clock\_gettime()}\footnote{Online documentation for \texttt{clock\_gettime()}: \url{http://linux.die.net/man/3/clock_gettime}} returns a time with nanosecond precision, but is only available on specific systems (e.g. Linux). Since the operating system used for the evaluation framework is Linux and nanosecond precision is desired, \texttt{clock\_gettime()} has been used for timing all operations.