\section{Evaluating and Choosing an Index Structure}
\label{sec:comparison}

\subsection{Measuring Efficiency}
\label{sec:measuring-efficiency}

One can measure the efficiency of an index structure by simply measuring the amount of time it takes perform dynamic operations and queries \cite{dynamic-data-structures}. In addition to execution time, one must also consider how much memory overhead is produced by the index structure. Such overhead may not be an issue for small data sets, but when your index structure becomes very large, it must be considered due to the issues discussed in Section \ref{sec:paging}. Measurements of index structure efficiency include:
\begin{itemize}
	\item \textbf{Structure Size} -- memory required to store the structure, relative to dataset size
	\item \textbf{Construction Time} -- how long it takes to construct a structure storing $n$ points
	\item \textbf{Dynamic Operation Execution Time} -- execution times of \texttt{insert} and \texttt{delete} operations
	\item \textbf{Query Execution Time} -- execution times of point, range or nearest neighbour queries
\end{itemize}
Big-Oh notation \cite{design-analysis-algorithms} refers to the worse case execution time of an algorithm, often with respect to the size of the input. This is used to bound the worst, average or expected execution time of an algorithm. This can be used to construct theoretical performance measurements for an index structure and if often used to guide the development and evaluation of index structures structures (e.g. in  \cite{splay-quadtree}).

The focus of bucket methods is to minimise I/O operations while still retaining well-balanced search trees. Therefore, in addition to the runtime of each operation, the number of I/O operations, or \textbf{page accesses}, is often measured (e.g. \cite{pk-tree, pyramid-tree, x-tree}). In conjunction with CPU processing time, page access count can give good insight into how efficient an index structure is for large datasets.

\subsection{Deciding on an Index Structure}
\label{sec:structure-decision}

While there are some structures which have been shown to be less efficient than others (e.g. R-tree when compared to X-tree \cite{x-tree}), the behaviour of these structures often varies with the data it is given. Some structures work better on low-dimensional, uniform data, while others are better for high-dimensional data. Table \ref{tab:comparison} in Appendix \ref{chap:supp-material} lists some of the index structures discussed in this review and their strengths/weaknesses.
In other words, there is no ``best" structure. Choosing an index structure to use is \textit{task-dependent}. There are many factors to consider when choosing an index structure, some of which are listed below. Note that this is by no means an exhaustive list.
\begin{itemize}
	\item how many points will the structure contain?
	\item how many dimensions does the data have?
	\item how frequently will the data be modified, if at all?
	\item do queries have to be performed near real-time or is it acceptable to wait a few minutes?
	\item what distributions will be input data be? will they uniform or highly skewed?
\end{itemize}

The strengths and weaknesses of the structures discussed in the review will now be summarised. B${}^{+}$-trees are very popular for one-dimensional data because they are fast, simple and have low memory overhead \cite{ubiquitous-btree}. Quadtrees, kd-trees and R-tree variants have good performance on multi-dimensional data (e.g. for geometric optimisation of a 3D scene \cite{kd-tree-gpu}) and are simple to implement (no complex invariants to maintain). However, performance starts to decrease as you increase the number of dimensions. There is a need for structures which can perform efficient queries in high-dimensional space, so structures such as the pyramid tree and PK-tree were developed \cite{pk-tree, pyramid-tree}. For even higher dimensions (e.g. $d > 10$), it has been shown that non-hierarchical methods may provide better performance (e.g. hashing, sequential scan, etc.).