\section{Evaluation}
\label{sec:evaluation}

One of the project's deliverables is an evaluation of the implemented index structure(s). Multiple evaluations will be performed to guide what needs to be achieved in each iteration of development and determine how efficient the final implementations at the end of the project are. As such, evaluation forms a large part of the project. This section details what measurements will be used to evaluate implementation performance, what the baselines are (i.e. what the implementations will be compared to) and what data will be used for the evaluation.

\subsection{Measures}

A variety of measurements have been used to evaluate the performance of index structures. Which measurements to use depend on the focus of the research and what problem the specific structure is trying to solve. Section \ref{sec:measuring-efficiency} discusses the ways index structures are commonly evaluated in the literature. This project is aimed towards accelerating index structures both algorithmically and by optimising the implementation itself. The performance analysis tools discussed in Section \ref{sec:development-tools} will be used to produce some of the chosen evaluation measures, which are:

\begin{itemize}
	\item \textbf{Total Execution Time} -- total time it took an operation to execute
	\item \textbf{Cache Hit Rate} -- rate of cache hits to misses ($\frac{\text{\# cache hits}}{\text{\# cache misses}}$)
	\item \textbf{Total Heap Memory} -- amount of heap memory used over the entire simulation
	\item \textbf{Peak Heap Memory} -- maximum amount of heap memory used at once by the simulation 
	\item \textbf{Total Memory} -- total amount of stack and heap memory used over the entire simulation
	\item \textbf{\# Heap Allocations} -- the amount of system calls made to allocate memory on the heap
	\item \textbf{CPU/IO Ratio} -- the ratio between time spent on computation and time spent on I/O operations (e.g. cache reads, heap allocations, etc.)
	\item \textbf{Speedup Factor} -- increase in speed when compared to another index structure (e.g. evaluation baselines). This is given by $\frac{t_1}{t_2}$, where $t_1$ and $t_2$ are the running times of the two index structures being compared.
\end{itemize}

One assumption being made about the target data is that it is dynamic, meaning points may be inserted or deleted at any time. For query performance, the evaluation will focus on the performance of point queries. Therefore, index structures will be evaluated using the performance of their \textbf{insert}, \textbf{delete} and \textbf{point query} operations.

A typical performance test will have the following steps:
\begin{enumerate}
	\item Incrementally build structure by adding each point in test dataset one-by-one
	\item Perform point queries with points in the structure and points that are not
	\item Interleave \texttt{insert} and \texttt{delete} operations with a number of point queries
\end{enumerate}
Using this process allows the \textbf{average} performance of large numbers of \texttt{insert}, \texttt{delete} and point query operations to be computed. How well the implementations perform with a mixture of operations can also be determined. For example, the contents of the cache may change rapidly due to different operations being performed consecutively, highlighting if cache locality has an impact on the performance of those implementations. More information on how points from the datasets are selected for the test operations will be given in the final report.

\subsection{Baseline}

There is little use in measuring the performance of the implemented index structures for the purposes of evaluation if there is nothing to compare the results to. As such, two baseline index structure implementations will be developed -- \textbf{sequential scan} and the \textbf{PR quadtree} (see Section \ref{sec:recursive-partition-structures}). The reason for choosing sequential scan as a baseline is that it's the na\"{i}ve brute-force approach to performing search. The PR quadtree is the basis of many index structures, both old and new. Thus, the structure is well-known in the field and makes a suitable baseline.

These baselines will be developed in C++ to match the technology used to develop the initially implemented index structure.

\subsection{Data}

Both synthetic, artificially generated data and a real dataset will be used for the evaluation. Three types of \textbf{synthetic} datasets will be generated -- uniform distributed, skewed and clustered. Multiple instances of these datasets will be used, with varying numbers of dimensions (e.g. 1, 2, 3, 5, 10, 50, 100). Using such datasets was inspired by Wang et al. and Berchtold et al., who used similar datasets to evaluate the performance of the PK-tree and pyramid tree respectively \cite{pk-tree, pyramid-tree}. The reason for testing the index structures with these datasets is that the variety of point distributions and number of dimensions should tease out which data the implementations struggle with and which they work well at.

The real dataset being used for the evaluation is the result of an \textbf{astrophysics turbulence simulation}, where a $600 \times 248 \times 248$ regular mesh was used to simulate ``three-dimensional radiation hydrodynamical calculations of ionization front instabilities" \cite{astrophysics-dataset}. Each point of the mesh has ten scalar fields, which include particle density, temperature and eight chemical species. 200 timesteps were recorded, each being approximately 126 to 128 years apart \cite{astrophysics-dataset}. For each evaluation, only \textbf{one timestep} of the simulation will be used at a time. Since some of the timesteps have different distributions, multiple tests with different timesteps may be performed.

The astrophysics was chosen because it is large (almost 37 million points) and has a high number of dimensions (10). The core focus of the project is high-dimensional data, so this data will provide a true test of how well the implementations perform with real instances of such data. Various resolutions of the dataset may be used for testing to reduce the amount of data being tested at any one point. Datasets with smaller resolutions will be constructed by \textbf{uniformly sampling} the points of the original simulation's mesh.

More information on how the synthetic datasets are generated and how the timesteps of the astrophysics turbulence simulation dataset are chosen will be in the final report.
